{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZilJeq2Qr4VO"
      },
      "outputs": [],
      "source": [
        "\n",
        "Collecte toutes les pages Wikip√©dia en fonction d'un wikicode\n",
        "\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "#Fonction\n",
        "def get_wiki_links(entity_id):\n",
        "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    wiki_links = []\n",
        "    try:\n",
        "        sitelinks = data['entities'][entity_id]['sitelinks']\n",
        "        for site, info in sitelinks.items():\n",
        "            wiki_links.append({\n",
        "                'site': site,\n",
        "                'title': info['title'],\n",
        "                'url': info['url']\n",
        "            })\n",
        "    except KeyError:\n",
        "        print(f\"No sitelinks found for entity: {entity_id}\")\n",
        "    return wiki_links\n",
        "\n",
        "# Example usage\n",
        "entity_id = 'Q3580827'  # Example entity (Douglas Adams)\n",
        "wiki_links = get_wiki_links(entity_id)\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "links = pd.DataFrame(wiki_links)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Deduction du plus long contenu\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_wikipedia_page(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        return f\"Failed to fetch page. Status code: {response.status_code}\"\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "def parse_wikipedia_page(soup):\n",
        "    # Extract the title\n",
        "    title = soup.find('h1', {'id': 'firstHeading'}).text\n",
        "\n",
        "    # Extract the summary\n",
        "    summary = ''\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        summary += paragraph.text\n",
        "    return summary\n",
        "\n",
        "links['LengthContent'] = links['url'].apply(lambda x: len(parse_wikipedia_page(fetch_wikipedia_page(x)).split(' ')))"
      ],
      "metadata": {
        "id": "LjdiOsZxtCBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}